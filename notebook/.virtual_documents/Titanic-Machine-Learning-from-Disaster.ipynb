import os
import itertools
import pickle

import numpy as np
import pandas as pd
import seaborn as sns 
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from sklearn.ensemble import VotingClassifier

from sklearn.model_selection import cross_val_score

from sklearn.metrics import f1_score, make_scorer, classification_report

from sklearn.model_selection import GridSearchCV 
from sklearn.model_selection import RandomizedSearchCV 





train = pd.read_csv('./data/train.csv')
test = pd.read_csv('./data/test.csv')

train['train_test'] = 1
test['train_test'] = 0
test['Survived'] = np.NaN
all_data = pd.concat([train, test])

%matplotlib inline
all_data.columns











#quick look at our data types & null counts 
train.info()





# An overview of the central tendencies of the numeric data
train.describe()


# Quick way of separating the numeric columns
train.describe().columns


# Separating the numeric and categorical columns as each will require a separate approach
df_numeric = train[['Age','SibSp','Parch','Fare']]
df_categorical = train[['Survived','Pclass','Sex','Ticket','Cabin','Embarked']]


# Creating a grid of subplots for the numeric columns
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))
axes = axes.flatten()  # Flatten 2D array to 1D

# Plot each histogram in a subplot
for i, col in enumerate(df_numeric.columns):
    plt.sca(axes[i])
    plt.hist(df_numeric[col])
    plt.title(col)

# Adjust layout for better spacing
plt.tight_layout()

# Show the plots
plt.show()





# A look at the correlation of our numeric columns
print(df_numeric.corr())
sns.heatmap(df_numeric.corr())


# A look at survival rate across Age, SibSp, Parch, and Fare 
pd.pivot_table(
    data = train,
    values = ['Age', 'SibSp', 'Parch', 'Fare'],
    index = 'Survived'
)


# Same grid of subplots as the one above but containing barplots with our categorical features
fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 10))
axes = axes.flatten()

# Plot each barplot in a subplot
for i, col in enumerate(df_categorical.columns):
    sns.barplot(x=df_categorical[col].value_counts().index, y=df_categorical[col].value_counts(), ax=axes[i])
    axes[i].set_title(col)

# Adjust layout for better spacing
plt.tight_layout()

# Show the plots
plt.show()





# Survival by Pcalss
pd.pivot_table(
    train, 
    index='Survived', 
    columns='Pclass', 
    values='Ticket', 
    aggfunc='count'
)


# Survival by Sex
pd.pivot_table(
    train, 
    index='Survived', 
    columns='Sex', 
    values='Ticket', 
    aggfunc='count'
)


# Survival by embarked
pd.pivot_table(
    train, 
    index='Survived', 
    columns='Embarked', 
    values='Ticket', 
    aggfunc ='count'
)








train['cabin_multiple'] = train.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))
train['cabin_multiple'].value_counts()


pd.pivot_table(
    train, 
    index='Survived', 
    columns='cabin_multiple', 
    values='Ticket', 
    aggfunc='count'
)


# Creates categories based on the cabin letter (n stands for null). In this case we will treat null values like a separate category.
train['cabin_letters'] = train.Cabin.apply(lambda x: str(x)[0])
print(train.cabin_letters.value_counts())


# Comparing surivial rate by cabin
pd.pivot_table(
    train, 
    index='Survived', 
    columns='cabin_letters', 
    values='Name', 
    aggfunc='count'
)


# Taking a better look a the ticket values. Numeric vs non numeric:
train['numeric_ticket'] = train.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)
train['ticket_letters'] = train.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.', '').replace('/', '').lower() if len(x.split(' ')[:-1]) > 0 else 0)
train['numeric_ticket'].value_counts()


# Lets us view all rows in dataframe through scrolling
train['ticket_letters'].value_counts()


# Difference in numeric vs non-numeric tickets in survival rate 
pd.pivot_table(
    train, 
    index='Survived',
    columns='numeric_ticket',
    values='Ticket',
    aggfunc='count'
)


# Survival rate across different ticket types 
pd.pivot_table(
    train,
    index='Survived',
    columns='ticket_letters',
    values='Ticket',
    aggfunc='count'
)


train.Name.head(50)


# Let's pull out the title as a separate feature with values of - Mr., Ms., Master. etc
train['name_title'] = train.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())
train['name_title'].value_counts()





# Setting up all categorical variables that will be used for both the training and the test sets 
all_data['cabin_multiple'] = all_data.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))
all_data['name_title'] = all_data.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())

# Engineered categorical features which ended up not being used
# all_data['cabin_letters'] = all_data.Cabin.apply(lambda x: str(x)[0])
# all_data['numeric_ticket'] = all_data.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)
# all_data['ticket_letters'] = all_data.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('/','').lower() if len(x.split(' ')[:-1]) > 0 else 0)

# Impute nulls for continuous data 
#all_data.Age = all_data.Age.fillna(training.Age.mean())
all_data.Age = all_data.Age.fillna(train.Age.median())
#all_data.Fare = all_data.Fare.fillna(training.Fare.mean())
all_data.Fare = all_data.Fare.fillna(train.Fare.median())

# Drop null 'Embarked' rows as they are of no relevance (2 instances in the tarining set and none in the testing set)
all_data.dropna(subset=['Embarked'], inplace=True)

# Attempted log norm of sibsp (not used)
#all_data['norm_sibsp'] = np.log(all_data.SibSp+1)
#all_data['norm_sibsp'].hist()

# log norm of fare (used)
all_data['norm_fare'] = np.log(all_data.Fare+1)
all_data['norm_fare'].hist()

# Converting fare to a categorical feature for pd.get_dummies()
all_data.Pclass = all_data.Pclass.astype(str)

# Createing dummy variables from categories (also can use OneHotEncoder)
all_dummies = pd.get_dummies(all_data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'norm_fare', 'Embarked', 'cabin_multiple', 'name_title', 'train_test']])
# all_dummies = pd.get_dummies(all_data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'cabin_multiple', 'name_title', 'train_test']])





# I want to make sure that the data remains as a pandas dataframe when splitting. Features get split based on the train_test flag from earlier:
X_train = pd.DataFrame(all_dummies[all_dummies.train_test == 1].drop(['train_test'], axis=1))
X_test = pd.DataFrame(all_dummies[all_dummies.train_test == 0].drop(['train_test'], axis=1))

# Same logic for training labels
y_train = pd.DataFrame(all_data[all_data.train_test==1].Survived)
y_train = y_train.squeeze()
print('y_train shape ->', y_train.shape)


# Now to Scale the data
scale = StandardScaler()
all_dummies_scaled = all_dummies.copy()
all_dummies_scaled[['Age', 'SibSp', 'Parch', 'norm_fare']] = scale.fit_transform(all_dummies_scaled[['Age', 'SibSp', 'Parch', 'norm_fare']])
# all_dummies_scaled[['Age', 'SibSp', 'Parch', 'Fare']] = scale.fit_transform(all_dummies_scaled[['Age', 'SibSp', 'Parch', 'Fare']])
all_dummies_scaled

X_train_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 1].drop(['train_test'], axis = 1)
X_test_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 0].drop(['train_test'], axis = 1)

y_train = all_data[all_data.train_test==1].Survived








def model_evaluation(models: dict, X_train, y_train):
    """
    This function uses Cross Validation to evaluate models' perfomence. Returns a report of the models' names and mean scores.
    """
    evaluation_report = {}
    for model_name, model in models.items():
        if model_name in ['KNeighbors Classifier']:
            f1_scorer = make_scorer(f1_score, average='weighted')
            cv = cross_val_score(model, X_train.values, y_train, cv=5, scoring=f1_scorer)  # converting X_train to np.array with .values to avoid warning
        else:
            cv = cross_val_score(model, X_train, y_train, cv=5, scoring='f1_weighted')

        evaluation_report[model_name] = cv.mean()
        
        cv_rounded = [f"{round(score * 100, 1)}%" for score in cv]
        cv_mean = f"{round(cv.mean() * 100, 1)}%"
        
        print(f"{model_name}:\n- CV F1 scores: {' | '.join([item for item in cv_rounded])}\n- CV mean: {cv_mean}\n")

    return evaluation_report


def model_optimisation(models: dict, X_train, y_train, params: dict) -> dict:
    '''
    This function uses dictionaries of models and parameters on which it will run a grid 
    search cross-validation to determine which option is the optimal one for each model.
    Returns a dictionary of models with their best estimators applied.
    '''
    tuned_models = {}
    
    for model_name, model in models.items():
        
        print(f"{model_name}:")
        
        f1_scorer = make_scorer(f1_score, average='weighted')  # Use weighted F1 score for multi-class classification
        
        tuned_model = GridSearchCV(
            model,
            param_grid=params[model_name],
            cv=5,
            scoring=f1_scorer,
            verbose=True,
            n_jobs=-1
        )
        
        tuned_model.fit(X_train, y_train)
        tuned_models[model_name] = [tuned_model.best_estimator_, tuned_model.best_score_]

        # Report model scores and best parameters
        print('- Best Grid Search F1 Score: ' + str(round(tuned_model.best_score_ * 100, 1)) + '%')
        print('- Best Parameters: ' + str(tuned_model.best_params_))
        print()
        
    return tuned_models


def train_test_classification_report(y_train, y_train_preds, y_test=None, y_test_preds=None):
    """
    The function prints a classification report for the train data and test data if available.
    """
    print(),
    print("--------------------  Train set scores  --------------------"),
    print(classification_report(y_train, y_train_preds))
    
    if y_test is not None and y_test_preds is not None:
        print(),
        print("--------------------  Test set scores  --------------------"),
        print(classification_report(y_test, y_test_preds))


def generate_submissions_csv(predictions, file_name):
    '''
    A function that export predictions to a CSV file in a format compatible with the Kaggle requirements
    '''
    # Create submissions dictionary in the format defined by Kaggle
    submissions_dict = {'PassengerId': test.PassengerId, 'Survived': predictions}
    submissions_df = pd.DataFrame(data=submissions_dict)

    # Setup export folder
    base_path = 'submissions'
    os.makedirs(base_path, exist_ok=True)

    # Generate CSV
    file_path = os.path.join(base_path, file_name)
    submissions_df.to_csv(file_path, index=False)

    print(f'File "{file_name}" saved to <./{base_path}>')


def save_model(model, file_name):
    """
    Save .pkl file
    """
    dir = 'models'
    path = os.path.join(dir, file_name)
    os.makedirs(dir, exist_ok=True)

    with open(path, "wb") as file_obj:
        pickle.dump(model, file_obj)
        
    print(f'File "{file_name}" saved to <./{dir}>')


def load_model(file_name):
    """
    Load .pkl file
    """
    dir = 'models'
    path = os.path.join(dir, file_name)
    with open(path, 'rb') as file:
        model = pickle.load(file)
        
    print(f'File "{file_name}" loaded')
    
    return model





models = {
    'Logistic Regression': LogisticRegression(max_iter=2000),
    'Decision Tree Classifier': DecisionTreeClassifier(random_state=1),
    'Random Forest Classifier': RandomForestClassifier(random_state=1),
    'GaussianNB': GaussianNB(),
    'KNeighbors Classifier': KNeighborsClassifier(),
    'SVC': SVC(probability=True),
    'XGB Classifier': XGBClassifier(random_state=1),
    'CatBoost Classifier': CatBoostClassifier(silent=True)
}





_ = model_evaluation(models, X_train, y_train)


_ = model_evaluation(models, X_train_scaled, y_train)








# Build model
voting_clf_model_tuples = [(model_name, model) for model_name, model in models.items()]
voting_clf = VotingClassifier(estimators=voting_clf_model_tuples, voting='soft')


# Evaluate model
_ = model_evaluation({'Voting Classifier': voting_clf}, X_train_scaled.values, y_train)





# Let's produce a file with preditions to use as a baseline
voting_clf.fit(X_train_scaled.values, y_train)
voting_clf_preds = voting_clf.predict(X_test_scaled.values).astype(int)

# Generate file
generate_submissions_csv(voting_clf_preds, '01_voting_clf_submission.csv')











models_to_remove = ['GaussianNB', 'Decision Tree Classifier']
for model in models_to_remove:
    if model in models:
        del models[model]





# Setting up a dictionary of parameter grids for each model that I'd like to try and tune
# Had to reduce the number of parameters for some models to not have overly long searches
model_params = {
    'Logistic Regression': {
        'max_iter' : [10, 50, 100, 1000],
        'penalty' : ['l1', 'l2'],
        'C' : np.logspace(-4, 4, 20),
        'solver' : ['liblinear']
        },
    # 'Random Forest Classifier': {
    #     'n_estimators': [100, 500, 1000], 
    #     'bootstrap': [True, False],
    #     'max_depth': [3, 5, 10, 20, 50, 75, 100, None],
    #     'max_features': ['auto', 'sqrt', 'log2'],
    #     'min_samples_leaf': [1, 2, 4, 10],
    #     'min_samples_split': [2, 5, 10]
    #     },
    'Random Forest Classifier': {
        'n_estimators': [100, 300, 450],
        'criterion':['gini', 'entropy'], 
        'bootstrap': [True],
        'max_depth': [10, 15],
        'max_features': ['sqrt', 20, 40],
        'min_samples_leaf': [1, 2],
        'min_samples_split': [.5, 2]
        },
    'KNeighbors Classifier': {
        'n_neighbors' : [3, 5, 7, 9, 12, 15],
        'weights' : ['uniform', 'distance'],
        'algorithm' : ['ball_tree', 'kd_tree'],
        'p' : [1, 2]
        },
    'SVC': [
        {'kernel': ['rbf'], 'gamma': [.1, .5, 1, 2, 5, 10], 'C': [.1, 1, 10]},
        {'kernel': ['linear'], 'C': [.1, 1, 10]},
        {'kernel': ['poly', 'linear', 'rbf'], 'degree' : [1, 2, 3], 'C': [.1, 1, 10]}
        ],
    'XGB Classifier': {
        'n_estimators': [500, 550],
        'colsample_bytree': [.5, .6, .75],
        'max_depth': [10, None],
        'reg_alpha': [1],
        'reg_lambda': [5, 10, 15],
        'subsample': [.55, .6, .65],
        'learning_rate':[.5],
        'gamma':[.25, .5, 1],
        'min_child_weight':[0.01],
        'sampling_method': ['uniform']
        },
    'CatBoost Classifier': {
        'iterations': [400, 500],
        'learning_rate': [0.01, 0.05, 0.1],
        'depth': [2, 4, 6],
        'l2_leaf_reg': [1, 2],
        'border_count': [64, 128]
    }
}





# Let's see if we can get some increase in performance
tuned_models = model_optimisation(models, X_train_scaled, y_train, model_params)





xgb_predictions = tuned_models['XGB Classifier'][0].predict(X_test_scaled).astype(int)
generate_submissions_csv(xgb_predictions, '02_xgb_tuned_submission.csv')





best_rf = tuned_models['Random Forest Classifier'][0].fit(X_train_scaled, y_train)
feat_importances = pd.Series(best_rf.feature_importances_, index=X_train_scaled.columns)
feat_importances.nlargest(10).plot(kind='barh')








tuned_model_tuples = [(model_name, model[0]) for model_name, model in tuned_models.items()]
voting_clf_all = VotingClassifier(estimators=tuned_model_tuples, voting='hard')
_ = model_evaluation({'Voting Classifier': voting_clf_all}, X_train_scaled, y_train)


tuned_model_tuples = [(model_name, model[0]) for model_name, model in tuned_models.items()]
voting_clf_all = VotingClassifier(estimators=tuned_model_tuples, voting='soft')
_ = model_evaluation({'Voting Classifier': voting_clf_all}, X_train_scaled, y_train)


tuned_model_tuples = [(model_name, model[0]) for model_name, model in tuned_models.items() if model_name not in ['Logistic Regression']]
voting_clf_all = VotingClassifier(estimators=tuned_model_tuples, voting='soft')
_ = model_evaluation({'Voting Classifier': voting_clf_all}, X_train_scaled, y_train)


tuned_model_tuples = [(model_name, model[0]) for model_name, model in tuned_models.items() if model_name not in ['Logistic Regression', 'KNeighbors Classifier']]
voting_clf_all = VotingClassifier(estimators=tuned_model_tuples, voting='soft')
_ = model_evaluation({'Voting Classifier': voting_clf_all}, X_train_scaled, y_train)





# Weights grid
combinations = itertools.product([1, 2], repeat=len(tuned_model_tuples))  # Generate all possible combinations of weights
combinations = [list(comb) for comb in combinations if len(set(comb)) != 1]  # Filter out the combinations where all elements are the same

# Voting classifier param grid
voting_classifier_params = {'Voting Classifier': {'weights': combinations, 'voting': ['soft', 'hard']}}

# Optimising the voting classifier
tuned_voting_clf = model_optimisation(
    {'Voting Classifier': voting_clf_all},
    X_train_scaled,
    y_train,
    voting_classifier_params
)





# Adding the Voting Classifier to the tuned models dictionary and sorting it to easily retrieve the best performing model
tuned_models = {**tuned_voting_clf, **tuned_models}
sorted_tuned_models = dict(sorted(tuned_models.items(), key=lambda item: -item[1][1]))

# Taking best model from dictionary
best_model_name, best_model_tuple = list(sorted_tuned_models.items())[0]
best_model, best_model_score = best_model_tuple
print(f"Best performing model -> {best_model_name} with anm F1 score of {round(best_model_score * 100, 1)}%")





# Generating final submission file
best_model_preds = best_model.predict(X_test_scaled).astype(int)
generate_submissions_csv(xgb_predictions, '03_optimised_model_submission.csv')

# Saving model
save_model(best_model, 'titanic_survival_classifier.pkl')
